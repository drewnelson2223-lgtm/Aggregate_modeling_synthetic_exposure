# Aggregate Loss Modeling: Tweedie vs Synthetic Exposure
### With Comprehensive Validation Framework

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Andrew_Nelson-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/andrew-nelson-)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![R Version](https://img.shields.io/badge/R-4.5.2-276DC3?style=flat&logo=r&logoColor=white)](https://www.r-project.org/)
[![Tests](https://img.shields.io/badge/Tests-105_passed_(97%25)-success)](tests/)
[![Validation](https://img.shields.io/badge/Validation-Cross--Validated-brightgreen)](docs/TESTING.md)

---

## üéØ TL;DR

**When exposure data is unavailable, Tweedie distribution modeling achieves 5.12% prediction error while traditional Compound Poisson-Gamma with synthetic assumptions fails with 905,421% error‚Äîa 177,000√ó performance difference confirmed through 105 automated tests with 97% pass rate and robust 10-fold cross-validation.**

| Approach | MAE | Cross-Val MAE | Tests Passed | Recommended |
|----------|-----|---------------|--------------|-------------|
| **CP-Gamma (Synthetic)** | 905,421% | N/A | 0/29 (0%) | ‚ùå No |
| **Tweedie (Preferred)** | 5.12% | 5.34% | 102/105 (97%) | ‚úÖ Yes |

---

## üìä Key Visualizations

### Actual vs Models: The 177,000√ó Performance Gap

![Model Comparison](reports/figures/model_comparison_standalone.png)

CP-Gamma predictions with synthetic exposure explode into the billions while actual losses remain in the millions. Tweedie tracks actuals with 5.12% MAE ‚Äî demonstrating why methodology must match available data.

### Tweedie Model Fit: Q-Q Diagnostic

![Tweedie Q-Q Plot](reports/figures/tweedie_qq.png)

Deviance residuals closely follow the theoretical normal distribution, confirming the Tweedie GLM is well-specified for this data. Minor departure in the upper tail is consistent with the heavy-tailed loss structure (p = 1.762).

---

## üî¨ Research Question

**Can traditional frequency-severity decomposition be salvaged through synthetic exposure assumptions when actual claim counts and exposure data are unavailable, or should actuaries adapt their methodology to work directly with aggregate losses?**

This project empirically tests whether forcing a Compound Poisson-Gamma structure using synthetic exposures (derived from earned premium) can produce reliable estimates, or whether direct aggregate modeling via Tweedie distributions better serves actuarial practice when facing incomplete data.

**Findings:** Synthetic assumptions catastrophically fail (905,421% error) even with industry-standard values, while adapted methodology (Tweedie GLM) achieves 5.34% cross-validated error. **Conclusion: Match methods to available data rather than force traditional approaches with unverified assumptions.**

---

## üíº Business Takeaways

### For Actuarial Practice

1. **When exposure data is missing, model aggregate losses directly**
   - Tweedie GLMs achieve professional-grade accuracy (5.34% error) without exposure counts
   - Never rely on synthetic assumptions for production estimates (demonstrated 177,000√ó performance gap)

2. **Severity-dominated loss structure** (power parameter p = 1.762)
   - Aggregate losses driven by few large claims, not claim frequency
   - Reinsurance strategies should prioritize severity management over frequency controls
   - Capital allocation should account for heavy-tailed company-level risk

3. **Complementary tail risk perspectives:**
   - **GEV (industry maxima):** Bounded tail (100-year return = $11.0M) ‚Üí regulatory stress testing
   - **GPD (company exceedances):** Heavy tail (infinite variance) ‚Üí reinsurance design and capital allocation
   - Diversification benefits at industry level vs. concentration risk at company level

4. **Premium scales proportionally with losses** (elasticity = 1.024)
   - Validates earned premium as exposure base when actual exposure unavailable
   - Supports experience rating and premium adequacy assessments

### For Model Development

1. **Comprehensive validation prevents catastrophic failures**
   - 105 automated tests with 97% pass rate ensure code correctness
   - Cross-validation (5.12% ‚Üí 5.34%) confirms robust generalization
   - Production-grade quality assurance applicable to any actuarial model

2. **Transparent methodology builds stakeholder confidence**
   - All code, tests, and validation publicly available
   - Reproducible results with clear documentation
   - Suitable for regulatory review and audit requirements

---

## üìä Project Overview

Using CAS Schedule P personal auto bodily injury data (1988-1997, 1,166 company-years), this analysis compares:

1. **Compound Poisson-Gamma** with synthetic exposure/claim counts (traditional approach forced onto incomplete data)
2. **Tweedie distribution** modeling of aggregate losses (methodology adapted to available data)
3. **Extreme Value Theory** (GEV for annual maxima, GPD for threshold exceedances)

The analysis includes a **production-grade validation framework** with 105 unit tests, 10-fold cross-validation, and comprehensive diagnostics‚Äîdemonstrating professional software engineering practices for actuarial research.

---

## üî• Key Results

### Model Performance

```
Tweedie Model:
‚îú‚îÄ In-sample MAE: 5.12%
‚îú‚îÄ Cross-validation MAE: 5.34% (10-fold)
‚îú‚îÄ Systematic bias: 0.34% (minimal)
‚îú‚îÄ Pseudo R¬≤: 0.9766 (97.7% deviance explained)
‚îî‚îÄ 177,000√ó better than CP-Gamma with synthetic exposure
```

### Model Specification

```
log(Œº) = Œ≤‚ÇÄ + Œ≤‚ÇÅ¬∑Year + Œ≤‚ÇÇ¬∑log(Premium)
where S ~ Tweedie(Œº, œÜ, p) with 1 < p < 2

Parameters:
‚îú‚îÄ Intercept: 54.59 (p < 0.0001)
‚îú‚îÄ Year: -0.0278 (p < 0.0001) ‚Üí -2.74% annual trend (1988-1997)
‚îú‚îÄ log(Premium): 1.024 (p < 0.0001) ‚Üí proportional scaling
‚îú‚îÄ Power (p): 1.762 ‚Üí severity-dominated structure
‚îî‚îÄ Dispersion (œÜ): 1.47
```

### Validation Status

‚úÖ **Data Quality:** 23/23 tests passed (100%)  
‚úÖ **Model Diagnostics:** 29/29 tests passed (100%)  
‚úÖ **Prediction Accuracy:** 29/29 tests passed (100%)  
‚úÖ **EVT Validation:** 21/24 tests passed (87.5%)  
‚úÖ **Overall:** 102/105 tests passed (97.1%)

### Extreme Value Analysis

**GEV (Annual Maxima):**
- Shape (Œæ): -0.33 ‚Üí Weibull (bounded tail)
- 100-year return: $11.0M (industry-wide maximum losses bounded)

**GPD (Threshold Exceedances):**
- Shape (Œæ): 0.82 ‚Üí Heavy tail (infinite variance)
- Company-specific extremes exhibit concentration risk

---

## üöÄ Quick Start

### Prerequisites

```r
# Required packages
install.packages(c("MASS", "tidyverse", "tweedie", "statmod", "evd", "testthat"))
```

### Run Analysis with Validation

```bash
# Clone repository
git clone https://github.com/drewnelson2223-lgtm/Aggregate_modeling_synthetic_exposure.git
cd Aggregate_modeling_synthetic_exposure

# Run complete analysis with validation in R
source("run_analysis_with_validation.R")
```

**Output:** Model results in `results/`, figures in `reports/figures/`, validation report in `results/validation_report.txt`

### Run Tests

```r
# Execute all 105 tests
source("tests/run_tests.R")
```

**Expected:** 102/105 tests passed (97.1%), 3 acceptable failures on synthetic data convergence

---

## üìÅ Repository Structure

```
Aggregate_modeling_synthetic_exposure/
‚îú‚îÄ‚îÄ üìÑ README.md
‚îú‚îÄ‚îÄ üìÑ run_analysis_with_validation.R    # Master pipeline with validation
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ utils.R                          # Shared utility functions
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_prep.R                   # Data loading and preparation
‚îÇ   ‚îú‚îÄ‚îÄ 02_cp_gamma_synth.R              # CP-Gamma (demonstration only)
‚îÇ   ‚îú‚îÄ‚îÄ 03_Tweedie_modeling.R            # Tweedie GLM (recommended)
‚îÇ   ‚îú‚îÄ‚îÄ 04_EVT_analysis.R               # GEV and GPD analysis
‚îÇ   ‚îú‚îÄ‚îÄ 05_comparison_visualization.R    # Comparison plots and summary
‚îÇ   ‚îî‚îÄ‚îÄ validation.R                     # Validation functions
‚îú‚îÄ‚îÄ üìÅ tests/                             # 105 automated tests
‚îÇ   ‚îú‚îÄ‚îÄ run_tests.R
‚îÇ   ‚îî‚îÄ‚îÄ testthat/
‚îÇ       ‚îú‚îÄ‚îÄ test-data-prep.R (23 tests)
‚îÇ       ‚îú‚îÄ‚îÄ test-tweedie.R (19 tests)
‚îÇ       ‚îú‚îÄ‚îÄ test-validation.R (29 tests)
‚îÇ       ‚îî‚îÄ‚îÄ test-evt.R (24 tests)
‚îú‚îÄ‚îÄ üìÅ docs/
‚îÇ   ‚îú‚îÄ‚îÄ TESTING.md                       # Comprehensive testing guide
‚îÇ   ‚îî‚îÄ‚îÄ TESTING_SUMMARY.md
‚îî‚îÄ‚îÄ üìÅ reports/
    ‚îú‚îÄ‚îÄ synthetic_exposures.pdf          # Full academic paper
    ‚îî‚îÄ‚îÄ figures/
        ‚îú‚îÄ‚îÄ model_comparison_standalone.png
        ‚îú‚îÄ‚îÄ tweedie_qq.png
        ‚îú‚îÄ‚îÄ annual_comparison.png
        ‚îú‚îÄ‚îÄ error_comparison.png
        ‚îú‚îÄ‚îÄ tweedie_residuals.png
        ‚îú‚îÄ‚îÄ gev_diagnostic.png
        ‚îî‚îÄ‚îÄ gpd_diagnostic.png
```

---

## üí° When to Use This Methodology

### Use Tweedie When:
- ‚úÖ Have aggregate loss and premium data
- ‚úÖ Missing exposure counts and claim-level data
- ‚úÖ Need reliable aggregate predictions
- ‚úÖ Want to infer frequency vs. severity dominance

### Use CP-Gamma When:
- ‚úÖ Have actual exposure and claim count data
- ‚úÖ Need separate frequency and severity estimates
- ‚úÖ Modeling individual claim severities

### Never:
- ‚ùå Force traditional methods with synthetic assumptions for production estimates
- ‚ùå Rely on point estimates without comprehensive validation
- ‚ùå Extrapolate historical trends (1988-1997) to current periods without validation

---

## ‚ö†Ô∏è Data Context & Limitations

**Data Period:** 1988-1997 CAS Schedule P (27-36 years old)

**What Remains Valid:**
- ‚úÖ Comparative methodology (Tweedie vs synthetic CP-Gamma)
- ‚úÖ Validation framework and testing practices
- ‚úÖ Structural relationships (severity dominance, proportional scaling)

**Recommendations for Current Use:**
1. Replicate with 2010-2024 data for current forecasting
2. Obtain actual exposure data when available (Schedule P Part 2, state filings)
3. Use validation framework as template for production models
4. Apply both GEV and GPD for comprehensive tail risk assessment

---

## üìö Documentation

- **[Full Paper (PDF)](reports/synthetic_exposures.pdf)** - Academic paper with complete methodology and results
- **[Testing Guide (TESTING.md)](docs/TESTING.md)** - How to run tests and validation
- **[Technical Summary (TESTING_SUMMARY.md)](docs/TESTING_SUMMARY.md)** - Detailed technical documentation

---

## üéì Academic Context

Suitable for:
- ‚úÖ Actuarial job portfolios (demonstrates technical depth + software engineering)
- ‚úÖ Graduate actuarial courses (advanced statistical modeling)
- ‚úÖ CAS/SOA student paper competitions (original research + validation)
- ‚úÖ Methodology demonstrations (comparative analysis best practices)

---

## üìÑ License

MIT License - see LICENSE file for details.

---

## üìß Contact

**Andrew Nelson**

- LinkedIn: [linkedin.com/in/andrew-nelson-](https://www.linkedin.com/in/andrew-nelson-)
- GitHub: [github.com/drewnelson2223-lgtm](https://github.com/drewnelson2223-lgtm)
- Email: Drewnelson2@verizon.net

---

## üôè Acknowledgments

- **Data Source:** Casualty Actuarial Society (CAS) Schedule P Database
- **R Packages:** `tweedie`, `evd`, `statmod`, `MASS`, `tidyverse`, `testthat`
- **Methodology:** J√∏rgensen (1987), Dunn & Smyth (2018), Embrechts et al. (1997)

---

## üìä Citation

```bibtex
@misc{nelson2025tweedie,
  author = {Nelson, Andrew},
  title = {Aggregate Loss Modeling: Tweedie vs Synthetic Exposure with Comprehensive Validation},
  year = {2025},
  url = {https://github.com/drewnelson2223-lgtm/Aggregate_modeling_synthetic_exposure},
  note = {105 automated tests, 97\% pass rate, 10-fold cross-validated}
}
```

---

<div align="center">

**‚≠ê If you find this useful, please star the repository! ‚≠ê**

[![GitHub stars](https://img.shields.io/github/stars/drewnelson2223-lgtm/Aggregate_modeling_synthetic_exposure?style=social)](https://github.com/drewnelson2223-lgtm/Aggregate_modeling_synthetic_exposure)

*Built with rigor. Validated with tests. Ready for production.*

</div>

avg_severity = mean(totalPaid, na.rm = TRUE)
) %>%
mutate(
yearOfLoss = yr,
frequency = total_claims
) %>%
select(yearOfLoss, total_claims, total_paid, avg_severity, frequency)
agg_summary <- bind_rows(agg_summary, summary_year)
}
# -----------------------------
# NFIP Claims Aggregation by Year (Memory-Efficient)
# -----------------------------
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
# -----------------------------
# Step 1: Define API endpoint
# -----------------------------
base_url <- "https://www.fema.gov/api/open/v2/FimaNfipClaims"
# -----------------------------
# Step 2: Function to pull claims for a specific year
# -----------------------------
pull_fema_claims_year <- function(year, top = 1000) {
skip <- 0
year_claims <- data.frame()
repeat {
url <- paste0(base_url, "?yearOfLoss=", year, "&$top=", top, "&$skip=", skip)
response <- GET(url)
if (status_code(response) != 200) {
stop(paste("Error fetching data for year", year))
}
data_json <- content(response, "text", encoding = "UTF-8")
df <- fromJSON(data_json)$FimaNfipClaims
if (length(df) == 0) break
year_claims <- bind_rows(year_claims, df)
n_new <- nrow(df)
cat("Year:", year, "Pulled", n_new, "records\n")
skip <- skip + n_new
}
return(year_claims)
}
# -----------------------------
# Step 3: Define years to pull
# -----------------------------
years <- 1978:2024  # Adjust as needed
# Initialize summary table
agg_summary <- data.frame(
yearOfLoss = integer(),
total_claims = integer(),
total_paid = numeric(),
avg_severity = numeric(),
frequency = integer()
)
# -----------------------------
# Step 4: Pull data year by year and aggregate
# -----------------------------
for (yr in years) {
claims <- pull_fema_claims_year(yr)
if (nrow(claims) == 0) next  # skip if no claims
# Clean and calculate total paid
claims <- claims %>%
mutate(
amountPaidOnBuildingClaim = as.numeric(amountPaidOnBuildingClaim),
amountPaidOnContentsClaim = as.numeric(amountPaidOnContentsClaim),
totalPaid = rowSums(select(., amountPaidOnBuildingClaim, amountPaidOnContentsClaim), na.rm = TRUE)
)
# Aggregate for this year
summary_year <- claims %>%
summarise(
total_claims = n(),
total_paid = sum(totalPaid, na.rm = TRUE),
avg_severity = mean(totalPaid, na.rm = TRUE)
) %>%
mutate(
yearOfLoss = yr,
frequency = total_claims
) %>%
select(yearOfLoss, total_claims, total_paid, avg_severity, frequency)
agg_summary <- bind_rows(agg_summary, summary_year)
}
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
# FEMA NFIP Claims API (OpenFEMA)
base_url <- "https://www.fema.gov/api/open/v2/FimaNfipClaims"
# Function to safely pull claims for a given year
pull_claims_for_year <- function(year, top = 1000) {
skip <- 0
year_data <- data.frame()
# Loop through pages until no more data
repeat {
# Build query URL
url <- paste0(
base_url,
"?yearOfLoss=", year,
"&$top=", top,
"&$skip=", skip
)
# API call
resp <- try(GET(url), silent = TRUE)
if (inherits(resp, "try-error") || status_code(resp) != 200) {
# Stop if API fails
message(paste("  → Skipping year", year, "- no records or API issue"))
return(NULL)
}
# Parse JSON
parsed <- fromJSON(content(resp, "text", encoding = "UTF-8"))$FimaNfipClaims
# Stop if no records returned
if (length(parsed) == 0) break
year_data <- bind_rows(year_data, parsed)
n_new <- nrow(parsed)
message(paste("  → Year", year, "pulled", n_new, "records"))
skip <- skip + n_new
}
# If after pagination there's no data, return NULL
if (nrow(year_data) == 0) return(NULL)
return(year_data)
}
# Years to process (adjust if desired)
years_to_process <- 1980:2025
# Prepare container for annual summaries
annual_summary <- data.frame(
yearOfLoss = integer(),
total_claims = integer(),
total_paid = numeric(),
avg_severity = numeric(),
frequency = integer()
)
# Loop through each year
for (yr in years_to_process) {
message(paste("Processing year:", yr))
claims <- pull_claims_for_year(yr)
# Skip if no data returned
if (is.null(claims)) next
# Clean numeric fields and calculate totalPaid
claims <- claims %>%
mutate(
yearOfLoss = as.integer(yearOfLoss),
amountPaidOnBuildingClaim = as.numeric(amountPaidOnBuildingClaim),
amountPaidOnContentsClaim = as.numeric(amountPaidOnContentsClaim),
totalPaid = rowSums(
select(., amountPaidOnBuildingClaim, amountPaidOnContentsClaim),
na.rm = TRUE
)
)
# Aggregate summary for this year
year_summary <- claims %>%
summarise(
total_claims = n(),
total_paid = sum(totalPaid, na.rm = TRUE),
avg_severity = mean(totalPaid, na.rm = TRUE)
) %>%
mutate(
yearOfLoss = yr,
frequency = total_claims
) %>%
select(yearOfLoss, total_claims, total_paid, avg_severity, frequency)
# Append to master summary
annual_summary <- bind_rows(annual_summary, year_summary)
}
# Print results
print(annual_summary)
# ----------------------------------
# Plotting Results
# ----------------------------------
ggplot(annual_summary, aes(x = factor(yearOfLoss), y = frequency)) +
geom_col(fill = "steelblue") +
labs(
title = "NFIP Claim Frequency by Year",
x = "Year of Loss",
y = "Number of Claims"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(annual_summary, aes(x = factor(yearOfLoss), y = avg_severity)) +
geom_col(fill = "darkred") +
labs(
title = "NFIP Claim Severity (Average Paid) by Year",
x = "Year of Loss",
y = "Average Claim Paid ($)"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(httr)
library(jsonlite)
library(dplyr)
# Base API
base_url <- "https://www.fema.gov/api/open/v2/FimaNfipClaims"
# Parameters
page_size <- 1000
skip <- 0
max_pages <- 10  # for testing; increase for full download
all_claims <- data.frame()
for (i in 1:max_pages) {
url <- paste0(base_url, "?$top=", page_size, "&$skip=", skip)
resp <- GET(url)
data_json <- content(resp, "text", encoding="UTF-8")
df <- fromJSON(data_json)$FimaNfipClaims
if (length(df) == 0) break
all_claims <- bind_rows(all_claims, df)
skip <- skip + nrow(df)
cat("Pulled page", i, ":", nrow(df), "records\n")
}
# Filter in R by yearOfLoss
all_claims$yearOfLoss <- as.numeric(all_claims$yearOfLoss)
claims_by_year <- all_claims %>%
filter(!is.na(yearOfLoss)) %>%
group_by(yearOfLoss) %>%
summarise(
total_claims = n(),
total_paid = sum(as.numeric(amountPaidOnBuildingClaim) + as.numeric(amountPaidOnContentsClaim), na.rm=TRUE),
avg_severity = mean(as.numeric(amountPaidOnBuildingClaim) + as.numeric(amountPaidOnContentsClaim), na.rm=TRUE)
)
print(claims_by_year)
# Make sure yearOfLoss is treated as a factor for plotting
claims_by_year$yearOfLoss <- factor(claims_by_year$yearOfLoss)
# -------------------------
# Frequency Plot
# -------------------------
ggplot(claims_by_year, aes(x = yearOfLoss, y = total_claims)) +
geom_col(fill = "steelblue") +
labs(
title = "NFIP Claim Frequency by Year",
x = "Year of Loss",
y = "Number of Claims"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# -------------------------
# Severity Plot
# -------------------------
ggplot(claims_by_year, aes(x = yearOfLoss, y = avg_severity)) +
geom_col(fill = "darkred") +
labs(
title = "NFIP Claim Severity (Average Paid) by Year",
x = "Year of Loss",
y = "Average Paid per Claim ($)"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# ----------------------------------------
# NFIP Claims Analysis by Year (Inflation-Adjusted)
# ----------------------------------------
# Load libraries
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
# -----------------------------
# Step 1: FEMA NFIP Claims API
# -----------------------------
base_url <- "https://www.fema.gov/api/open/v2/FimaNfipClaims"
page_size <- 1000  # number of records per page
# -----------------------------
# Step 2: CPI Data (2024 USD reference)
# -----------------------------
cpi_data <- data.frame(
year = 1980:2024,
cpi = c(
82.4, 87.4, 90.9, 96.5, 99.6, 103.9, 107.6, 109.6, 113.6, 118.3,
124.0, 130.7, 136.2, 140.3, 144.5, 148.2, 152.4, 156.9, 160.5, 165.3,
170.3, 177.1, 184.0, 188.9, 195.3, 201.6, 207.3, 214.5, 220.5, 225.7,
229.6, 232.9, 236.7, 237.0, 240.0, 246.8, 252.0, 258.8, 264.7, 271.0,
276.7, 281.9, 287.3, 292.0, 296.8
)
)
ref_cpi <- cpi_data$cpi[cpi_data$year == 2024]
# -----------------------------
# Step 3: Function to pull claims in pages
# -----------------------------
pull_claims_chunk <- function(skip, top = 1000) {
url <- paste0(base_url, "?$top=", top, "&$skip=", skip)
resp <- GET(url)
if (status_code(resp) != 200) return(NULL)
df <- fromJSON(content(resp, "text", encoding = "UTF-8"))$FimaNfipClaims
if (length(df) == 0) return(NULL)
return(df)
}
# -----------------------------
# Step 4: Pull data in chunks and aggregate
# -----------------------------
skip <- 0
all_summary <- data.frame()
repeat {
claims <- pull_claims_chunk(skip, page_size)
if (is.null(claims)) break
# Clean and compute totalPaid
claims <- claims %>%
mutate(
yearOfLoss = as.numeric(yearOfLoss),
amountPaidOnBuildingClaim = as.numeric(amountPaidOnBuildingClaim),
amountPaidOnContentsClaim = as.numeric(amountPaidOnContentsClaim),
totalPaid = rowSums(select(., amountPaidOnBuildingClaim, amountPaidOnContentsClaim), na.rm=TRUE)
) %>%
filter(!is.na(yearOfLoss))
# Aggregate by year
yearly <- claims %>%
group_by(yearOfLoss) %>%
summarise(
total_claims = n(),
total_paid = sum(totalPaid, na.rm=TRUE),
avg_severity = mean(totalPaid, na.rm=TRUE)
) %>%
mutate(frequency = total_claims)
# Append to master summary
all_summary <- bind_rows(all_summary, yearly)
skip <- skip + nrow(claims)
cat("Pulled", nrow(claims), "records; total summary rows:", nrow(all_summary), "\n")
}
# -----------------------------
# Step 5: Aggregate across duplicate years
# -----------------------------
claims_by_year <- all_summary %>%
group_by(yearOfLoss) %>%
summarise(
total_claims = sum(total_claims),
total_paid = sum(total_paid),
avg_severity = total_paid / total_claims,
frequency = sum(frequency)
) %>%
ungroup()
# -----------------------------
# Step 6: Merge CPI and adjust for inflation
# -----------------------------
claims_by_year <- claims_by_year %>%
left_join(cpi_data, by = c("yearOfLoss" = "year")) %>%
mutate(
total_paid_inflation_adjusted = total_paid * (ref_cpi / cpi),
avg_severity_inflation_adjusted = avg_severity * (ref_cpi / cpi)
)
# -----------------------------
# Step 7: Plot Frequency
# -----------------------------
claims_by_year$yearOfLoss <- factor(claims_by_year$yearOfLoss)
ggplot(claims_by_year, aes(x = yearOfLoss, y = frequency)) +
geom_col(fill = "steelblue") +
labs(
title = "NFIP Claim Frequency by Year",
x = "Year of Loss",
y = "Number of Claims"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# -----------------------------
# Step 8: Plot Inflation-Adjusted Severity
# -----------------------------
ggplot(claims_by_year, aes(x = yearOfLoss, y = avg_severity_inflation_adjusted)) +
geom_col(fill = "darkgreen") +
labs(
title = "NFIP Claim Severity by Year (Inflation-Adjusted to 2024 USD)",
x = "Year of Loss",
y = "Average Paid per Claim ($, 2024 USD)"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
install.packages(c("arrow", "dplyr", "lubridate", "readr"))
library(httr)
library(jsonlite)
library(dplyr)
library(future.apply)
install.packages(c(future.apply))
nstall.packages("future.apply")
Install.packages("future.apply")
install.packages("future.apply")
library(future.apply)
library("future.apply")
library(httr)
library(jsonlite)
library(dplyr)
library(future.apply)
# Parallel plan: adjust workers to your CPU (2–4 recommended)
plan(multisession, workers = 4)
pull_nfip_state_year_parallel <- function(
start_year = 2000,
end_year = as.integer(format(Sys.Date(), "%Y")),
page_size = 20000,  # larger pages -> fewer API calls
checkpoint_dir = "nfip_checkpoints"
) {
base_url <- "https://www.fema.gov/api/open/v2/FimaNfipClaims"
states <- c("AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA",
"HI","ID","IL","IN","IA","KS","KY","LA","ME","MD",
"MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ",
"NM","NY","NC","ND","OH","OK","OR","PA","RI","SC",
"SD","TN","TX","UT","VT","VA","WA","WV","WI","WY")
# Create checkpoint directory
if (!dir.exists(checkpoint_dir)) dir.create(checkpoint_dir)
# Function to pull a single state across all years
pull_state <- function(st) {
message("Starting state: ", st)
state_results <- list()
for (y in start_year:end_year) {
skip <- 0
repeat {
filt <- paste0("(yearOfLoss eq ", y, ") and (state eq '", st, "')")
resp <- GET(base_url, query = list(
"$top" = page_size,
"$skip" = skip,
"$filter" = filt
))
# HTTP error
if (http_error(resp)) {
warning("HTTP ", status_code(resp), " | year=", y, " state=", st)
break
}
txt <- content(resp, "text", encoding = "UTF-8")
if (nchar(txt) == 0) break
parsed <- tryCatch(
fromJSON(txt, flatten = TRUE),
error = function(e) NULL
)
if (is.null(parsed) || !"FimaNfipClaims" %in% names(parsed)) break
batch <- parsed$FimaNfipClaims
# Safe exit condition
if (is.null(batch) || !is.data.frame(batch) || nrow(batch) == 0) break
# Summarize batch
batch_summary <- batch %>%
mutate(
total_claim_paid =
coalesce(as.numeric(amountPaidOnBuildingClaim), 0) +
coalesce(as.numeric(amountPaidOnContentsClaim), 0) +
coalesce(as.numeric(amountPaidOnIncreasedCostOfComplianceClaim), 0)
) %>%
summarise(
claim_count = n(),
total_paid  = sum(total_claim_paid, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
state     = st,
loss_year = y
)
state_results[[length(state_results)+1]] <- batch_summary
skip <- skip + page_size
}
# Save checkpoint per state per year
if (length(state_results) > 0) {
saveRDS(
bind_rows(state_results),
file = file.path(checkpoint_dir, paste0("nfip_", st, ".rds"))
)
}
}
message("Finished state: ", st)
return(bind_rows(state_results))
}
# Run all states in parallel
all_states <- future_lapply(states, pull_state)
# Combine into a single data frame
results <- bind_rows(all_states) %>%
group_by(state, loss_year) %>%
summarise(
claim_count = sum(claim_count),
total_paid  = sum(total_paid),
.groups = "drop"
) %>%
mutate(
avg_claim_amount = ifelse(claim_count > 0,
total_paid / claim_count,
NA_real_)
)
return(results)
}
nfip_state_year <- pull_nfip_state_year_parallel(2000)
glimpse(nfip_summary)
glimpse(nfip_state_year)
saveRDS(nfip_state_year, "nfip_state_year_2000_present.rds")
# ============================================================================
# MASTER SCRIPT: TWO-PART ANALYSIS
# Run complete analysis pipeline from data loading to visualization
# ============================================================================
# Clear environment
rm(list = ls())
# Load required packages
cat("Loading required packages...\n")
suppressPackageStartupMessages({
library(MASS)         # Load first to avoid conflicts
library(tidyverse)
library(tweedie)
library(statmod)
library(evd)
})
# Set options
options(scipen = 999)  # Avoid scientific notation
set.seed(42)           # Reproducibility
cat("✅ Packages loaded\n\n")
# ============================================================================
# RUN ANALYSIS PIPELINE
# ============================================================================
cat("Starting analysis pipeline...\n\n")
# Step 1: Data Preparation
cat("STEP 1/5: Data Preparation\n")
source("src/01_data_preparation.R")
setwd("C:/Users/Drewn/Aggregate_modeling_synthetic_exposure")
source("run_analysis_with_validation (1).R")
edit("run_analysis_with_validation (1).R")
source("run_analysis_with_validation (1).R")
source("run_analysis_with_validation (1).R")
source("run_analysis_with_validation.R")
source("run_analysis_with_validation.R")
source("run_analysis_with_validation.R")
source("run_analysis_with_validation.R")
list.files(pattern = "run_analysis")
source("run_analysis_with_validation.R")
file.edit("run_analysis_with_validation.R")
source("run_analysis_with_validation.R")
list.files("src")
source("run_analysis_with_validation.R")
install.packages("tweedie")
source("run_analysis_with_validation.R")
source("run_analysis_with_validation.R")
